import re
import time
import json

import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import BitsAndBytesConfig
from sentence_transformers import SentenceTransformer

from src.database import RetrievalDatabase

class LLMResponseGenerator:
    def __init__(self, db : RetrievalDatabase, encoder : SentenceTransformer, llm_model: str = "unsloth/mistral-7b-instruct-v0.3-bnb-4bit"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–Ω–∫–æ–¥–µ—Ä–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.
        """
        self.db = db
        self.encoder = encoder
        self.tokenizer = AutoTokenizer.from_pretrained(llm_model)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            llm_model, quantization_config=bnb_config, device_map='cuda'
        )
        self.model = torch.compile(self.model)

    def format_llm_response(self, response_text: str) -> dict:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç, –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –æ—Ç LLM, –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON.
        :param response_text: –û—Ç–≤–µ—Ç –æ—Ç LLM –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏.
        :return: –°–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏ 'answer', 'reasoning' –∏ 'sources'.
        """
        try:
            response_data = json.loads(response_text)
        except json.JSONDecodeError:
            response_data = {
                "answer": -1,
                "reasoning": "",
                "sources": []
            }
            return response_data

        answer = response_data.get("answer", "")

        if isinstance(answer, str):
            match = re.match(r"\{(\d+)\}", answer)
            if match:
                answer = match.group(1)

        reasoning = response_data.get("reasoning", "").strip() + "\nModel: mistral-7b-instruct-v0.3-bnb-4bit"

        sources = response_data.get("sources", [])

        formatted_response = {
            "answer": answer,
            "reasoning": reasoning,
            "sources": sources
        }

        return formatted_response


    def generate_response(self, query: str, top_k: int = 5) -> str:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ FAISS + –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å LLM.
        """
        start_time = time.time()

        query_embedding = self.encoder.encode(query, convert_to_numpy=True).reshape(1, -1)

        print(f"–ï–Ω–∫–æ–¥–∏–Ω–≥ –∑–∞–Ω—è–ª: {time.time() - start_time:.4f} —Å–µ–∫")
        retrieved_docs = self.db.search(query_embedding, top_k=top_k)

        context = "\n\n".join([f"{doc['title']} ({doc['url']}): {doc['content']}" for doc in retrieved_docs][::-1])
        if len(context) > 4097:
            context = context[-4096:] # –¥–≤–∏–≥–∞–µ–º –±–æ–ª–µ–µ –±–ª–∏–∑–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫ –Ω–∞—á–∞–ª—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

        response_format = """{
            "answer": {–ù–æ–º–µ—Ä –æ—Ç–≤–µ—Ç–∞, –µ—Å–ª–∏ –Ω–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ - -1},
            "reasoning": {–ö—Ä–∞—Ç–∫–æ –æ–±–æ—Å–Ω—É–π –æ—Ç–≤–µ—Ç, —Ç—É—Ç –Ω–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Å—ã–ª–∫–∏!},
            "sources": {—Å–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –Ω–µ –±–æ–ª–µ–µ 3—Ö, –µ—Å–ª–∏ –æ–Ω–∏ –≤–æ–æ–±—â–µ —Ç—Ä–µ–±—É—é—Ç—Å—è}
        }"""

        prompt = f"–ò—Å–ø–æ–ª—å–∑—É—è —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é: \n\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n{context}\n\n–û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å, –ø—Ä–∏ —ç—Ç–æ–º —Ç–≤–æ–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, –∞ –∏–º–µ–Ω–Ω–æ:\n{response_format}\n–í–æ–ø—Ä–æ—Å: {query}\n–û—Ç–≤–µ—Ç:\n"
        with torch.inference_mode():
            input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids.to('cuda')

            output = self.model.generate(input_ids, max_new_tokens=256)
            response = self.tokenizer.decode(output[0], skip_special_tokens=True)

        formatted_response = self.format_llm_response(response.split('–û—Ç–≤–µ—Ç:\n')[1])


        print(f"üîç –ü–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–Ω—è–ª–∏ {time.time() - start_time:.4f} —Å–µ–∫")
        return formatted_response


if __name__ == "__main__":
    db = RetrievalDatabase(1024)

    generator = LLMResponseGenerator(db)

    # üîπ –ó–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    query = "–í –∫–∞–∫–æ–º –≥–æ–¥—É –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –ò–¢–ú–û –±—ã–ª –≤–∫–ª—é—á—ë–Ω –≤ —á–∏—Å–ª–æ –ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ –†–æ—Å—Å–∏–∏?\n1. 2007\n2. 2009\n3. 2011\n4. 2015"
    response = generator.generate_response(query)
    print(response)

    # –°—Ç—Ä–µ—Å—Å —Ç–µ—Å—Ç
    start_time = time.time()
    for i in range(100):
        query = "–í –∫–∞–∫–æ–º –≥–æ–¥—É –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –ò–¢–ú–û –±—ã–ª –≤–∫–ª—é—á—ë–Ω –≤ —á–∏—Å–ª–æ –ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ –†–æ—Å—Å–∏–∏?\n1. 2007\n2. 2009\n3. 2011\n4. 2015"
        response = generator.generate_response(query)
    print(f"üîç –ü–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–Ω—è–ª–∏ {time.time() - start_time:.4f} —Å–µ–∫")
    #print("\nü§ñ –û—Ç–≤–µ—Ç –æ—Ç LLM:")
    #print(response)
